# Factuality and Hallucination Detection Framework

## Problem statement

The project is about detecting and quantifying hallucinations in answers generated by large language models. The idea is to design a systematic way to check how much an answer is drifting away from factual content, and to do this without fully depending on big ground truth datasets or manual expert checking.

The original statement for the project is roughly as follows:

- LLMs can generate answers which look correct but are actually false or partially made up.
- Most current methods for hallucination detection need expensive labelled data, external knowledge bases, or human verification.
- The question is whether we can build a reliable hallucination detector using signals from the model itself, simple consistency checks, and light-weight verification.

This project is my attempt to build such a small framework.

## How I understand the problem

When an LLM answers a question, it usually mixes factual statements with some extra details. Some of those details may be fully correct, some may be unverifiable, and some may simply be wrong.  

So the task is not only “Is this whole answer correct or wrong?”, but more like:

- Break the answer into smaller factual pieces.
- Check each piece.
- Give a score saying how much of the answer is hallucinated.

Also, the requirement asks to avoid heavy ground truth datasets. So the design should rely mainly on:

- The model itself acting as a critic, and
- Simple processing logic around the answer.

## What hallucination means here (short)

In this project, I am using the following view of hallucination:

- A **supported** claim is clearly true according to common knowledge or provided context.
- An **unverifiable** claim is something that might be true, but cannot be clearly checked from context or general facts.
- A **hallucinated** claim is likely false, contradictory, or clearly invented.

So “hallucination” here is at the level of individual factual claims, not at the level of the full answer. The framework then gives a numeric score based on how many such claims are hallucinated.

## Approach I took

I tried to keep the implementation simple and transparent. Everything is in one Python file (`code.py`).

The steps are:

1. **Answer generation**

   - Take a question (and optionally some context).
   - Use the Gemini API to generate a detailed answer.
   - For generation I allow some temperature so that the model can produce more varied content, which also increases the chance of hallucinations.

2. **Claim extraction**

   - Split the answer into sentence-level units.
   - Each sentence is treated as a candidate factual claim.
   - This is done by a simple regular expression split on `.`, `?`, `!`. It is not perfect, but it is deterministic and does not require another model call.

3. **Claim classification**

   - For each set of claims, I again call Gemini, but now in a “critic” mode.
   - I give the question, optional context, and the list of claims.
   - I ask the model to return one line per claim in the format:
     `index | claim | label | reason`
   - The label is forced to be one of:
     - `supported`
     - `unverifiable`
     - `hallucinated`
   - The code parses these lines and stores them as a structured list.

4. **Hallucination score**

   - For one answer, the hallucination score is defined as:

     `hallucination_score = (# of claims labeled 'hallucinated') / (total # of claims)`

   - The score is in `[0, 1]`.  
     - `0.0` means no hallucinated claims were detected for that answer.  
     - Values closer to `1.0` mean a larger fraction of hallucinated claims.

5. **Multiple answers and multiple questions**

   - In the final version, the script does not stop at a single answer.
   - There is a list of questions (some factual, some open-ended, some intentionally tricky).
   - For each question, the code generates multiple samples (for example 3 different answers with higher temperature).
   - For each sample, it runs the full pipeline and computes a hallucination score.
   - At the end, the script prints a small summary per question:
     average score, minimum score, and maximum score over all samples.

This gives a small but useful picture of how stable the model is for different types of questions, and how often it tends to hallucinate when asked for more detailed answers.

## Code structure

Everything is inside `code.py`.

The main functions are:

- `generate_answer(example)`  
  Calls Gemini to produce an answer for one question.

- `extract_claims(question, answer, context)`  
  Splits the answer into sentence-level claims.

- `classify_claims(question, claims, context)`  
  Asks Gemini (in critic mode, with low temperature) to label each claim as supported / unverifiable / hallucinated, and to give a short reason.

- `hallucination_score(classified_claims)`  
  Computes the final score for a single answer.

- `run_example(example)`  
  Runs all steps for one answer and prints detailed logs.

- `run_multiple_questions(questions, num_samples_per_question)`  
  Runs the pipeline for several questions and several samples per question, and collects all results.

In the end, the script also dumps everything into a JSON file for later analysis.

## What kind of output this project produces

There are two main types of output.

1. **Console logs**

   For each answer, the script prints:

   - The question
   - The generated answer
   - The list of claims
   - The label and reason for each claim
   - The hallucination score for that answer  
   - A final summary per question with average / min / max score over all samples

   This is useful for directly reading what is happening.

2. **JSON file**

   At the end of `code.py`, all results are written into a file called `all_results.json`.  

   For each answer, it stores:

   - `id` (question number and sample number)
   - `question`
   - `answer`
   - `claims` (each with `claim`, `label`, `reason`)
   - `hallucination_score`

   A small example snippet (taken from one of the runs) looks like this:

   ```json
   {
     "id": "q1_s1",
     "question": "Who is the current CEO of Google and in which year was Google founded?",
     "answer": "The current CEO of Google is Sundar Pichai. Google was founded in 1998.",
     "claims": [
       {
         "claim": "The current CEO of Google is Sundar Pichai",
         "label": "supported",
         "reason": "This is a widely known and accurate fact."
       },
       {
         "claim": "Google was founded in 1998.",
         "label": "supported",
         "reason": "This is a widely known and accurate historical fact."
       }
     ],
     "hallucination_score": 0.0
   }
